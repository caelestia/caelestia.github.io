# First encounter with convex sets

From Shastri, Ch.2, though this is not the focus of the book.

The *affine* structure of $\mathbb{R}^d$ can be understood as follows. Given a finite subset $\\\{x_1, x_2, \cdots, x_n\\\}$ of $\mathbb{R}^d$, we say $\sum\lambda_ix_i$ is an *affine combination* of $x_i$'s if $\sum\lambda_i=0$.

The *affine hull* $\operatorname{aff}A$ of a subset $A\subset\mathbb{R}^d$ is then defined as all affine combinations in $A$. It is also clear how to define the notions of *affine subspace* and *affine independent subsets*.

It follows that

- $\operatorname{aff}A$ is an affine subspace of $\mathbb{R}^d$, and is of course the smallest one containing $A$. 

- If $V\subset\mathbb{R}^d$ is an affine subspace, $V-x$ is a linear subspace for all $x\in A$. We then have a well-defined dimension $\dim V:=\dim(V-x)$.

- $x_1, x_2,\cdots, x_n$ are affinely independent iff $x_2-x_1, x_3-x_1,\cdots, x_n-x_1$ are linearly independent. This implies $n\leq d+1$.

- $\dim V+1$ is the size of any maximal affinely independent subset of $V$.

**Theorem 1.(General Position)** A subset $A\subset\mathbb{R}^d$ is said to be in general position if any $k$-subset of $A$ is affinely independent for $k\leq d+1$. Let $A=\\\{x_1, x_2, \cdots, x_n\\\}$. We have

1. Given any $\varepsilon>0$, there exists $y_1,y_2,\cdots,y_n$ such that $\vert y_i-x_i\vert <\varepsilon$, and $y_i$'s are in general position.
1.  If $A$ is in general position, then there exists $\varepsilon>0$ such that $y_i$'s are in general position whenever $\vert y_i-x_i\vert <\varepsilon$, $\forall i$.

> Proof of 1. Induction on $n$. $n=1$ trivial. Suppose $y_1,\cdots,y_{n-1}$ is in general position. We need $y_n\notin$ some finite union of affine subspaces with $\dim\leq d-1$. The complement of this union is a finite intersection of *open* dense subsets, so is dense as well.

> Proof of 2. Expanding this condition as a finite collection of $\det$ functions, we see this is an "open" condition.

---

Given a finite subset $\\\{x_1, x_2, \cdots, x_n\\\}$ of $\mathbb{R}^d$, we say $\sum\lambda_ix_i$ is an *convex combination* of $x_i$'s if $\sum\lambda_i=1$ and $\lambda_i\geq 0$ for all $i$. We then define the convex hull $\operatorname{conv}A$ and convex subsets.

Define a $n$-simplex as the convex hull of $n+1$ affinely independent points. These are of course (affinely) isomorphic to the standard $n$-simplex $\Delta_n$.

**Theorem 2.(CarathÃ©odory)** Every $x\in\operatorname{conv}A$ lies in some $k$-simplex with vertices in $A$, where $k\leq d$.

> Proof. Equivalently, we need $x$ is a convex combination of $d+1$ points in $A$. Suppose $x=\sum_{i=1}^{d+2}\lambda_ix_i$ is a convex combination and all $\lambda_i>0$.
>
> As $x_1,\cdots,x_{d+2}$ are affinely dependent, we have $\sum\mu_ix_i=0$ with $\sum\mu_i=0$, and there is some $\mu_i>0$. We then find $\alpha=\operatorname{argmin}_i\\\{\lambda_i/\mu_i:\mu_i>0\\\}$. It follows that
>
> $$x=\sum_{i=1}^{d+2}\lambda_ix_i=\sum_{i\neq\alpha}\left(\lambda_i-\mu_i\frac{\lambda_\alpha}{\mu_\alpha}\right)x_i,$$
>
> a convex combination of $d+1$ points.

**Corollary 3.** If $A$ is compact then $\operatorname{conv}A$ is compact.

> Proof. We have shown the surjectivity of the map
>
> $$A^{d+1}\times\Delta_d\longrightarrow\operatorname{conv}A.$$

It is also clear that if $A$ is convex then the closure $\overline A$ is also convex. Since, by continuity, the above map induces

$$\overline{A}^{d+1}\times\Delta_d\longrightarrow\overline{\operatorname{conv}A}=\overline{A},$$

so $\operatorname{conv}\overline{A}\subset\overline{A}$.

---

We now introduce the notion of *supporting half-spaces*. Affine linear functions take the form $f(x)=\langle x,v\rangle+a$, where $\langle-,-\rangle$ is the standard inner product. Suppose $v\neq 0$, $H(f)=\\\{x:f(x)=0\\\}$ defines the hyperplanes and $H^+(f)=\\\{x:f(x)\geq 0\\\}$ defines the (closed) half-spaces.

Fix a convex subset $A$. $H^+$ is called a supporting half-space for $A$ if $A\subset H^+$. $H$ is called a supporting hyperplane if $H\cap A\neq\varnothing$ and $A\subset H^+$.

- Equivalently, $H(f)$ is a supporting hyperplane if

$$-\alpha=\min_{x\in A}\langle x,v\rangle$$

- If $A$ is a convex subset, every $x\notin\overline{A}$ can be separated from $A$ by a hyperplane. Indeed, by convexity, there is a $u\in\overline{A}$ minimizing the distance $\vert u-x\vert $. Let $H$ be the hyperplane perpendicular to $x-u$ and passing through $u$. If $x\in H^-$, minimality implies $\overline{A}$ lies entirely in $H^+$. A translation of $H$ to the midpoint of $x$ and $z$ thus separates $A$ from $x$.

As a consequence,

**Proposition 4.** Every closed convex set in $\mathbb{R}^d$ is the intersection of its supporting half-spaces.

**Definition.** View a convex set $A$ as a subset of the affine subspace $\operatorname{aff}A$. Then we can talk about its *relative interior* $\operatorname{ri}A$ and *relative boundary* $\operatorname{rb}A$. 

We can also define $\dim A:=\dim\operatorname{aff}A$.

- $\operatorname{rb}=\overline{A}\setminus\operatorname{ri}A$.
- If $A\neq\varnothing$, we have $\operatorname{ri}A$ is non-empty as well. Indeed, $A$ must contain some $\dim A$-simplex.

**Theorem 5.** Let $A$ be a convex set in $\mathbb{R}^d$. Then every $x\in A\setminus\operatorname{ri}A$ is contained in some supporting hyperplane for $A$.

> Proof. WLOG assume $x=0$. It suffices to prove the case $\operatorname{aff}A=\mathbb{R}^d$, as the supporting plane can be easily extended from $\operatorname{aff}A$ to $\mathbb{R}$. When $d=0$, $1$, there is nothing to prove. We will use induction for $d\geq 2$.
>
> ---
>
> If $d=2$, consider the map (a composition of inclusion and quotient):
> 
> $$A\setminus 0\longrightarrow\mathbb{R}^2\setminus 0\longrightarrow S^1.$$
>
> Since $\dim A>1$, $A\setminus 0$ is connected, so the image in $S^1$ is an arc $\gamma$.
> 
> If the central angle of $\gamma$ is greater than $\pi$, then there is a triangle in $\gamma$, and hence $A$, with $0$ in its interior, contradicting the fact that $x\notin\operatorname{ri}A$. So there must be a line $0\in L\subset\mathbb{R}^2$ with $A\subset L^+$.
>
> ---
>
> If $d>2$, consider a similar map
>
> $$A\cap\mathbb{R}^2\setminus 0\longrightarrow\mathbb{R}^2\setminus 0\longrightarrow S^1,$$
>
> where $\mathbb{R}^2$ is regarded as a subset of $\mathbb{R}^d$. Note that $A':=A\cap\mathbb{R}^2$ is an intersection of convex sets, hence convex. Two things can happen now:
>
> - $\dim(A')\leq 1$, or
> - $\dim(A')=2$. As before, $A'\setminus 0$ is connected and is mapped to an arc in $S^1$ with central angle $\leq\pi$.
>
> In either case, there is a line $0\in L\subset\mathbb{R}^2$ such that $A'\subset L^+$. Construct the quotient
> 
> $$q:\mathbb{R}^d\longrightarrow\mathbb{R}^d/L\simeq\mathbb{R}^{d-1}.$$
>
> Since $q$ is linear, $q(A)$ is convex. We've shown that $0\notin\operatorname{int}q(A)$. So by induction hypothesis, there is some supporting hyperplane $H$ for $q(A)$ in $\mathbb{R}^d/L$. This clearly lifts to $q^{-1}(H)$, a supporting hyperplane for $A$.

---
Post date: 2025/02/21 \
[Home page](https://caelestia.github.io)